import os

import torch
import numpy as np
from torch.autograd import Variable

from torch import nn


def dynamic_import(path):
    components = path.split('.')
    mod = __import__(components[0])
    for comp in components[1:]:
        mod = getattr(mod, comp)
    return mod


def recursive_detach(items):
    if isinstance(items, torch.Tensor):
        return items.detach()
    elif isinstance(items, list):
        for i, item in enumerate(items):
            items[i] = recursive_detach(item)
    else:
        return items


def init_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.normal_(m.weight, std=0.01)
        torch.nn.init.normal_(m.bias, std=0.01)
    elif type(m) == nn.Conv1d:
        torch.nn.init.normal_(m.weight, std=0.01)
        torch.nn.init.normal_(m.bias, std=0.01)


def recursive_to_cpu(value):
    """
    Shallow copy here for lists.

    :param value:
    :return:
    """
    if isinstance(value, torch.Tensor):
        # most features generated by network is gpu tensor
        # print('value is tensor')
        return value.cpu().detach()
    elif isinstance(value, list):
        # if is list of gpu tensor
        # print('value is list')
        for i in range(len(value)):
            value[i] = recursive_to_cpu(value[i])
    elif isinstance(value, tuple):
        value = list(value)
        for i in range(len(value)):
            value[i] = recursive_to_cpu(value[i])
    return value


def recursive_wrap_data(data, output_device):
    """
    recursively wrap tensors in data into Variable and move to device.

    :param data:
    :return:
    """
    if output_device == 'cpu':
        return data
    if isinstance(data, list):
        for i in range(len(data)):
            data[i] = recursive_wrap_data(data[i], output_device)
    elif isinstance(data, torch.Tensor):
        return Variable(data.cuda(output_device), requires_grad=False)
    return data


def change_ext(path: str, ext: str):
    """
    Changes ext in filename specified by path.
    ext should be like '.xxx' or 'xxx' in '.xxx'.
    """
    if not ext.startswith('.'):
        ext = '.' + ext
    return os.path.splitext(path)[0] + ext


def list_of_list_categorize(list_of_list, list_pos_as_key=0,
                            key_in_item=False,
                            ret_zipped=False,
                            ret_as_dict=True, reverse=False):
    """
    Change a list into dict with list at position of list_pos_reference as the key list.

    :param list_of_list:
    :param list_pos_as_key:
    :param key_in_item:
    :param ret_zipped: if False, value for each key is list of sub lists. else value for each key is zipped version of sub lists.
    :param ret_as_dict: if False, return keys_list, values_list.
    keys_list will be sorted, if `reverse` is True, sort to descending, otherwise ascending

    :param reverse: only considered when ret_as_dict is False.
    :return: dict(if ret_as_dict) or (keys, values)(if not ret_as_dict)
    """
    assert 0 <= list_pos_as_key < len(list_of_list)
    list_len = len(list_of_list[0])
    try:
        for lst in list_of_list:
            assert len(lst) == list_len
    except AssertionError:
        print('length not same!')
        print([len(lst) for lst in list_of_list])
        raise AssertionError
    if key_in_item:
        key_list = list_of_list[list_pos_as_key]
    else:
        key_list = list_of_list.pop(list_pos_as_key)
    zipped = tuple(zip(*list_of_list))
    dict_out = {key: [] for key in key_list}
    for i, key in enumerate(key_list):
        dict_out[key].append(zipped[i])
    if not ret_zipped:
        for key in dict_out.keys():
            dict_out[key] = list(zip(*dict_out[key]))
    if ret_as_dict:
        return dict_out
    else:
        keys = list(dict_out.keys())
        keys.sort(reverse=reverse)
        return keys, [dict_out[key] for key in keys]


def try_format_dict_with_path(dict_in, path, fmt_elem):
    """
    If value with path exists in dict_in, format it with fmt_elem using % format.
    Returns True if successfully formatted.
    """
    item = dict_in
    for p in path[:-1]:
        if p not in item:
            return False
        item = item[p]
    if path[-1] not in item:
        return False
    try:
        item[path[-1]] = item[path[-1]] % fmt_elem
        # print('formatted %s' % str(item[path[-1]]))
    except Exception:
        print('%s can not be formatted with %s' % (str(item[path[-1]]), fmt_elem))
        return False
    return True


def recursive_to_tensor(item, dtype=None):
    if isinstance(item, np.ndarray):
        item = torch.tensor(item, dtype=dtype)
    elif isinstance(item, list):
        for idx in range(len(item)):
            item[idx] = recursive_to_tensor(item[idx], dtype)
    return item


def recursive_to_ndarray(item, dtype=None):
    if isinstance(item, torch.Tensor):
        item = item.numpy()
        if dtype is not None:
            item = item.astype(dtype)
    elif isinstance(item, list):
        for idx in range(len(item)):
            item[idx] = recursive_to_ndarray(item[idx], dtype)
    return item


def recursive_stack(item):
    if isinstance(item, list):
        if all([isinstance(t, torch.Tensor) for t in item]):
            return torch.stack(item, dim=0)
        for idx in range(len(item)):
            item[idx] = recursive_stack(item[idx])
    return item


def recursive_zip(original, item=None, path=None):
    """
    Only zip the deepest iterable

    :param original:
    :param item:
    :param path:
    :return: hierarchical list structure
    """
    if item is None:
        # a representative of the structure to be zipped
        item = original[0]
        # return if nothing can be zipped
        if not isinstance(item, list):
            return original
        path = []
    zipped_list = []
    for pos, item_ in enumerate(item):
        # print('pos')
        # print(pos)
        # print('item_')
        # print(item_)
        # tuples are viewd as a whole and are not recursively processed!
        if isinstance(item_, list):
            zipped_list.append(recursive_zip(original, item_, path + [pos]))
        else:
            inner_zip = []
            for zip_input_idx in range(len(original)):
                item__ = original[zip_input_idx]
                for spec in path:
                    item__ = item__[spec]
                inner_zip.append(item__[pos])
            zipped_list.append(inner_zip)
    return zipped_list


def recursive_flatten_batch(item, cat_dim):
    """
    first dim is batch dim,
    and will concat along cat_dim
    """
    assert cat_dim != 0
    if isinstance(item, tuple):
        item = list(item)
    if isinstance(item, list):
        for idx in range(len(item)):
            item[idx] = recursive_flatten_batch(item[idx], cat_dim)
    elif isinstance(item, torch.Tensor):
        item = torch.moveaxis(item, cat_dim, 1)
        shape = list(item.shape)
        new_shape = [shape[0] * shape[1]] + shape[2:]
        item = item.reshape(new_shape)
        item = torch.moveaxis(item, 0, cat_dim-1)
        return item
    elif isinstance(item, np.ndarray):
        item = np.moveaxis(item, cat_dim, 1)
        shape = list(item.shape)
        new_shape = [shape[0] * shape[1]] + shape[2:]
        item = item.reshape(new_shape)
        item = np.moveaxis(item, 0, cat_dim-1)
        return item
    return item
